{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "url = \"https://api.ioda.inetintel.cc.gatech.edu/v2/entities/query\"\n",
    "start = time.time()\n",
    "country_to_region = {}\n",
    "lock = threading.Lock()  # to protect shared dict\n",
    "\n",
    "\n",
    "def fetchCountries():\n",
    "  params = {\n",
    "      \"entityType\": \"country\"\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "      \"accept\": \"*/*\"\n",
    "  }\n",
    "  response = requests.get(url, headers=headers, params=params)\n",
    "  print(response.json())\n",
    "  country_to_code = {country[\"name\"]:country[\"code\"] for country in response.json()[\"data\"]}\n",
    "  country_names = [country[\"name\"] for country in response.json()[\"data\"]]\n",
    "  print(country_to_code)\n",
    "  print(len(country_to_code))\n",
    "  return country_to_code\n",
    "\n",
    "country_to_code = fetchCountries()\n",
    "error=[]\n",
    "\n",
    "def fetch(country_name,country_code):\n",
    "  params = {\n",
    "      \"entityType\": \"region\",\n",
    "      \"relatedTo\": f\"country/{country_code}\"\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "      \"accept\": \"*/*\"\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers, params=params)\n",
    "  if response.status_code != 200:\n",
    "    error.append((response.status_code,country_name))\n",
    "\n",
    "  data = response.json()\n",
    "  region_names = [region[\"name\"] for region in data[\"data\"]]\n",
    "  with lock:\n",
    "    country_to_region[country_name] = region_names\n",
    "\n",
    "\n",
    "\n",
    "#for country_code in country_to_code.values():\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(fetch, country_name,country_code) for country_name, country_code in country_to_code.items()]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "print(country_to_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table database in notion for all countries\n",
    "\n",
    "from notion_client import Client\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Initialize Notion client\n",
    "NOTION_TOKEN = os.getenv('NOTION_TOKEN')\n",
    "#DATABASE_ID =  userdata.get('DATABASE_ID')\n",
    "\n",
    "notion = Client(auth=NOTION_TOKEN)\n",
    "\n",
    "# Load .env from the working directory\n",
    "load_dotenv()\n",
    "\n",
    "# Read MAIN_PAGE_ID from environment, fallback to colab userdata if not set\n",
    "main_page_id = os.getenv(\"MAIN_PAGE_ID\") \n",
    "created_database_ids = []\n",
    "\n",
    "\n",
    "def clean_multiselect_value(value):\n",
    "    return value.strip().title() if value else \"Unknown\"\n",
    "\n",
    "def get_existing_databases(page_id: str):\n",
    "    \"\"\"\n",
    "    Get all child databases on a given Notion page.\n",
    "\n",
    "    Returns:\n",
    "        dict: {database_title: database_id}\n",
    "    \"\"\"\n",
    "    databases = {}\n",
    "    has_more = True\n",
    "    start_cursor = None\n",
    "\n",
    "    while has_more:\n",
    "        #print(\"===\",page_id)\n",
    "        page = notion.pages.retrieve(page_id=page_id)\n",
    "        #print(\"-----\",page)\n",
    "        response = notion.blocks.children.list(\n",
    "            block_id=page[\"id\"],\n",
    "            start_cursor=start_cursor\n",
    "        )\n",
    "\n",
    "        for block in response[\"results\"]:\n",
    "            if block[\"type\"] == \"child_database\":\n",
    "                title = block[\"child_database\"][\"title\"].strip()\n",
    "                databases[title] = block[\"id\"]\n",
    "\n",
    "        has_more = response.get(\"has_more\", False)\n",
    "        start_cursor = response.get(\"next_cursor\")\n",
    "\n",
    "    return databases\n",
    "\n",
    "\n",
    "\n",
    "def create_database_link(country_name, region_list, existing_databases):\n",
    "    try:\n",
    "        if country_name in existing_databases:\n",
    "            print(f\"Database link for {country_name} already exists\")\n",
    "            return existing_databases[country_name]\n",
    "\n",
    "        # Create database as a child of main page\n",
    "        database = notion.databases.create(\n",
    "            parent={\"type\": \"page_id\", \"page_id\": main_page_id},\n",
    "            title=[{\"type\": \"text\", \"text\": {\"content\": country_name}}],\n",
    "            properties={\n",
    "                \"Company / Data source\": {\"title\": {}},\n",
    "                \"Region\": {\"rich_text\": {}},\n",
    "                \"Description\": {\"rich_text\": {}},\n",
    "                \"Comments on Web Scraping\": {\"rich_text\": {}},\n",
    "                \"Link\": {\"rich_text\": {}},\n",
    "                \"Scraping code created\": {\"rich_text\": {}},\n",
    "                \"Challenges\": {\"multi_select\": {}},\n",
    "                \"Data Type\": {\"multi_select\": {}},\n",
    "                \"Scraping Frequency\": {\"multi_select\": {}},\n",
    "                \"Status\": {\"multi_select\": {}}\n",
    "            }\n",
    "        )\n",
    "        db_id = database[\"id\"]\n",
    "\n",
    "        print(f\"Created database for {country_name}\")\n",
    "        return db_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database link for {country_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Get all existing databases on main page\n",
    "existing_databases = get_existing_databases(main_page_id)\n",
    "\n",
    "# Run concurrently\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for cn, rl in country_to_region.items():\n",
    "        futures.append(\n",
    "            executor.submit(create_database_link, cn, rl, existing_databases)\n",
    "          )\n",
    "    for future in as_completed(futures):\n",
    "        db_id = future.result()\n",
    "        if db_id:\n",
    "            created_database_ids.append(db_id)\n",
    "\n",
    "print(\"Created database links:\", created_database_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for web Scraping power outages company data by country and region_list\n",
    "\n",
    "def build_prompt(country):\n",
    "    return f\"\"\"You are a data scraper assistant.\n",
    "Return a JSON array of all **power supply companies** data operating in {country} as a list.\n",
    "\n",
    "For each company, strictly return JSON objects in this format:\n",
    "\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"Company / Data source\": \"Company Name\",\n",
    "    \"Region\": \"Area in which the company operates\",\n",
    "    \"Description\": \"What kind of power outage data the site provides (if no data, mention 'No outage data available')\",\n",
    "    \"Comments on Web Scraping\": \"Step-by-step manual scraping method or 'Not applicable' if no data\",\n",
    "    \"Power Outage Link\": \"URL of the official company page where outage data is (if none, use 'N/A'), Please provide the specific page not homepage\",\n",
    "    \"Website\": \"URL of the homepage of the official company website\",\n",
    "    \"Official Government Website URL\": \"URL of the official government website page where we got this company info\",\n",
    "    \"Scraping code created\": \"no\",\n",
    "    \"Challenges\": \"Challenges faced in scraping the data (or 'No challenges' if none)\",\n",
    "    \"Data Type\": \"Future | Historical | Excel | None\",\n",
    "    \"Scraping Frequency\": \"what is the frequency of data update? daily | weekly | monthly\",\n",
    "    \"Status\": \"Not Started\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "⚠️ Very Important:\n",
    "- Include **all companies** in {country} even if they do not provide outage data.\n",
    "- also scrape website if its in non english language.\n",
    "- Return only a valid JSON array, no explanation text outside JSON.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for using above prompt to scrape data\n",
    "\n",
    "\n",
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://models.github.ai/inference\",\n",
    "#     api_key=userdata.get('OPENAI_KEY'),\n",
    "# )\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_KEY'),\n",
    ")\n",
    "for country, _ in country_to_region.items():\n",
    "    prompt = build_prompt(country)\n",
    "    ASSISTANT = \"You are a data scraper assistant. Your job is to search internet to get power outage data. Go to power companies page and provide correct url of power outage pages and data asked for.\"\n",
    "    response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[{\"type\": \"web_search\"}],\n",
    "    input=prompt,\n",
    ")\n",
    "text = response.output[-1].content[0].text\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"given this chatgpt response api data. Extract scrapped json data from it.Only return json string which i can convert to python dict directly donot add any additional string\"},\n",
    "        {\"role\": \"user\", \"content\": response.output[-1].content[0].text},\n",
    "    ],\n",
    ");\n",
    "\n",
    "#print( response)\n",
    "# Get JSON output\n",
    "#print(response.output[-1].content)\n",
    "json_output = response.choices[0].message.content\n",
    "#print(json_output)\n",
    "data = json.loads(json_output)   # Python dict/list\n",
    "#print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data from above web scraper and update corresponding notion table\n",
    "\n",
    "import re\n",
    "from notion_client import Client\n",
    "\n",
    "\n",
    "NOTION_TOKEN =os.getenv('NOTION_TOKEN')\n",
    "DATABASE_ID =  os.getenv('DATABASE_ID')\n",
    "\n",
    "notion = Client(auth=NOTION_TOKEN)\n",
    "\n",
    "\n",
    "def clean_multiselect_value(value: str) -> str:\n",
    "    \"\"\"Remove special characters not allowed in Notion multi_select values.\"\"\"\n",
    "    if not value:\n",
    "        return \"N/A\"\n",
    "    # Keep only alphanumerics, spaces, hyphens, and underscores\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9 _-]\", \"\", value)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def page_exists(company_name, region):\n",
    "    query = notion.databases.query(\n",
    "        database_id=DATABASE_ID,\n",
    "        filter={\n",
    "            \"and\": [\n",
    "                {\n",
    "                    \"property\": \"Company / Data source\",\n",
    "                    \"title\": {\"equals\": company_name}\n",
    "                },\n",
    "                {\n",
    "                    \"property\": \"Region\",\n",
    "                    \"rich_text\": {\"equals\": region}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    return len(query.get(\"results\", [])) > 0\n",
    "\n",
    "\n",
    "for company in data:   # loop over JSON list\n",
    "    company_name = company[\"Company / Data source\"]\n",
    "    region = company[\"Region\"]\n",
    "\n",
    "    if page_exists(company_name, region):\n",
    "        print(f\"Skipping duplicate: {company_name} ({region})\")\n",
    "        continue\n",
    "    notion.pages.create(\n",
    "        parent={\"database_id\": DATABASE_ID},\n",
    "        properties={\n",
    "        \"Company / Data source\": {\"title\": [{\"text\": {\"content\": company[\"Company / Data source\"]}}]},\n",
    "        \"Region\": {\"rich_text\": [{\"text\": {\"content\": company[\"Region\"]}}]},\n",
    "        \"Description\": {\"rich_text\": [{\"text\": {\"content\": company[\"Description\"]}}]},\n",
    "        \"Comments on Web Scraping\": {\"rich_text\": [{\"text\": {\"content\": company[\"Comments on Web Scraping\"]}}]},\n",
    "        \"Link\": {\"rich_text\": [{\"text\": {\"content\": company[\"Link\"]}}]},\n",
    "        \"Scraping code created\": {\"rich_text\": [{\"text\": {\"content\": company[\"Scraping code created\"]}}]},\n",
    "        \"Challenges\": {\"multi_select\": [{\"name\": clean_multiselect_value(company[\"Challenges\"])}]},\n",
    "        \"Data Type\": {\"multi_select\": [{\"name\": clean_multiselect_value(dt.strip())} for dt in company[\"Data Type\"].split(\"|\") if dt.strip()]},\n",
    "        \"Scraping Frequency\": {\n",
    "            \"multi_select\": [\n",
    "                {\"name\": clean_multiselect_value(company.get(\"Scraping Frequency\", \"Once\").capitalize())}\n",
    "            ]\n",
    "        },\n",
    "        \"Status\": {\"multi_select\": [{\"name\": clean_multiselect_value(company[\"Status\"])}]}\n",
    "        }\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
